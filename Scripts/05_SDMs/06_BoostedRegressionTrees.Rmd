---
title: "Gradient Boosted Trees"
author: "Denisse Fierro Arcos"
date: "2023-12-01"
output: 
  github_document:
    toc: true
    html_preview: false
---

# Boosted Regression Trees via `SDMtune`

Boosted Regression Trees (BRTs) is a popular machine learning algorithm across many areas of study, and it has become increasingly used by ecologists to predict distribution of species. BRTs produce a large number of simple trees, with each tree optimising the performance of the previous one. A single simple tree will likely lead to a rather weak performance, but when multiple trees are combined, they boost the overall predictive performance of the model.  
  
In this project, we will use Boosted Regression Trees as one of the model algorithms used in our Species Distribution Model ensemble to estimate the distribution of crabeater seals in the recent past.  
  
## Loading libraries

```{r, message=F, warning=F}
library(tidyverse)
library(SDMtune)
library(gbm)
library(stars)
library(sf)
library(cmocean)
library(cowplot)
library(prg)
source("useful_functions.R")
```
  
## Setting up notebook

Selecting (or creating) an output folder for Random Forest results.  
  
```{r}
#Location of folder for outputs
out_folder <- "../../SDM_outputs/BoostedRegressionTrees/Mod_match_obs"
#If folder does not exist, create one
if(!dir.exists(out_folder)){
  dir.create(out_folder, recursive = T)
}

#Get path to files containing data
file_list <- list.files("../../Environmental_Data/", pattern = "Indian", full.names = T)
```
   
## Loading mean environmental conditions from ACCESS-OM2-01

This dataset includes the mean environmental conditions per month (November and December) over the entire period of study (1981 to 2013). Since Boosted Regression Trees are not affected my multicollinearity, we will include all variables available in the model.  
  
```{r}
mean_model <- read_csv("../../Environmental_Data/ACCESS-OM2-01/All_values_month_ACCESS-OM2-01_env_vars.csv") %>% 
  mutate(month = as.factor(month))

#List of categorical variables
cat_vars <- "month"

mean_model_baked <- prep_pred(mean_model, cat_vars)
```
    
## Loading layers for plotting
We will extract this layer from the `rnaturalearth` package. We will then reproject this layer to South Polar Stereographic (`EPSG 3976`).  
  
```{r}
#Loading layer
antarctica <- rnaturalearth::ne_countries(continent = "Antarctica",
                                          returnclass = "sf") %>% 
  #Transforming to South Polar Stereographic
  st_transform(3976)
```
  
## Loading environmental data from ACCESS-OM2-01 and setting up variables

We will use the datasets created in the notebook `02_Merging_background_presence_data.Rmd` located within the `Scripts/05_SDMs` folder. These datasets include the crabeater seal observations, background points, and environmental data.  
  
We will also define categorical and continuous explanatory variables.  
  
## Environmental variables matching observations 
Since multicollinearity is not an issue for Random Forest, we will include all ACCESS-OM2-01 outputs that match the environmental variables available in observational datasets.  
  
The variable `month` will be included as an ordinal factor in our analysis.  
  
```{r}
#Loading data
mod_match_obs <- read_csv(str_subset(file_list, "match")) %>% 
  #Setting month as factor and ordered factor
  mutate(month = as.factor(month))
```
  
### Splitting data into testing and training
The `prep_data` function in the `useful_functions` script will be used to split our data and to apply all necessary transformations.
  
We will then transform the data into SWD ("samples with data") format, which is the required format for inputs used in the `SDMtune` library.   
   
```{r}
#Getting training data
mod_match_obs <- prep_data(mod_match_obs, cat_vars, split = F)

#Applying SWD format to model data
model_data <- mod_match_obs %>% 
  select(!year) %>% 
  sdm_format() %>% 
  trainValTest(test = 0.25, only_presence = F, seed = 42)
```
  
## Modelling
Boosted Regression Trees have five hyperparameters that can be adjusted to improve model performance:  
1. `distribution` - Name of the distribution to use in the model. We will use the default value: `bernoulli` as we have binary data.  
2. `n.trees` - Total number of trees to be used in an ensemble.  
3. `interaction.depth` - Maximum depth (i.e., maximum number of nodes or branches) an individual tree can have.  
4. `shrinkage` - Commonly known as the *learning rate*. This determines the contribution of each tree to the final outcome.  
5. `bag.fraction` - The proportion of data used in selecting variables for tree expansion.  
  
Here, we use the `optimizeModel` function from the `SDMtune` library to test various values combinations for these hyperparameters, except `distribution`, which we will leave as the deafult. The `optimizeModel` function will a list of models tested ranked by their performance, so we will keep the first model returned.  
  
```{r, eval = F}
#Train model
default_model <- train(method = "BRT", data = model_data[[1]])

#Find number of predictors/features used
n_features <- ncol(model_data[[1]]@data)

# Define the hyperparameters to test
hyp_parm <- list(n.trees = seq(n_features*10, n_features*100, length.out = 10),
                 #maximum depth per tree
                 interaction.depth = 1:10,
                 #learning rate
                 shrinkage = c(1e-4, 1e-3, 1e-2, 0.1, 0.2, 0.3, 0.4, 0.5, 1),
                 bag.fraction = seq(0.25, 1, by = 0.25))

# Genetic algorithm that searches for best model parameters
optimised_model <- optimizeModel(default_model, hypers = hyp_parm, metric = "auc", 
                                 test = model_data[[2]], seed = 42)

#We will keep the best performing models based on AUC
best_mod_match_obs <- optimised_model@models[[1]]

#Save model
best_mod_match_obs %>% 
  saveRDS(file.path(out_folder, "best_BRT_mod_match_obs.rds"))

#Check model parameters
best_mod_match_obs
```
  
```{r, echo = F}
#Loading best initial model to avoid performing grid search of parameters
best_mod_match_obs <- readRDS(file.path(out_folder, "best_BRT_mod_match_obs.rds"))
```
  
## Variable importance
  
```{r}
#Calculating importance
var_imp_mod_match_obs <- varImp(best_mod_match_obs)
#Plotting results
plotVarImp(var_imp_mod_match_obs)
```
  
`SST` and `SIC` are the two most important variables identified in the model, similar to MaxEnt results. `month`, the slope of the seafloor (`bottom_slope_deg`) and the long-term presence of pack ice (`lt_pack_ice`) are the three least important variables, which coincides with results from Random Forest and MaxEnt. We can check model performance by plotting ROC curves and TSS value.  
  
## Jacknife test
We can check which environment variable contributes the most/least to the Boosted Regression Trees results when testing against the training and testing datasets.  

```{r}
jk_mod_match_obs <- doJk(best_mod_match_obs, metric = "auc", test = model_data[[2]])
jk_mod_match_obs
```
  
### Plotting Jacknife results
Results calculated from training dataset.  
  
```{r}
plotJk(jk_mod_match_obs, type = "train", ref = auc(best_mod_match_obs))
```
  
The `month`, long-term presence of pack ice (`lt_pack_ice`) and the slope of the seafloor (`bottom_slope_deg`) are the three variables that contribute the least when used by themselves, which coincides with results from the variable importance. On the other hand, `SIC`, `SST`, distance to the sea ice edge (`dist_ice_edge_km`) and to the continental shelf (`dist_shelf_km`) are the variables that contribute the most by themselves. Results suggest that removing any of variables included in the model will not negatively affect model performance, but we will check if this is also true when the model is applied to a testing dataset.  
    
```{r}
plotJk(jk_mod_match_obs, type = "test", ref = auc(best_mod_match_obs, test = model_data[[2]]))
```
  
The `month` and the slope of the seafloor (`bottom_slope_deg`) continue to be the two variable that contribute the least to the model, and the contribution of depth is also low. However, the difference in contribution across variables now has a smaller difference between the largest and smallest contributors. We can see that removing most variables from the model, except `month` and `bottom_slope_deg`, would negatively affect performance. We will calculate AUC and TSS values and then test if it is possible to remove any variables without affecting model performance to simplify the model.
  
## ROC curves
  
```{r}
plotROC(best_mod_match_obs, test = model_data[[2]])
```
  
Based on AUC values, Boosted Regression Trees have a slightly lower performance than Random Forests (test = 0.852), but it performs better than the MaxEnt model, which had an AUC value of 0.65.  
  
## TSS
This is a measure of predictive accuracy, which captures the proportion of correctly classified cells as true absences (specificity) or true presences (sensitivity). The TSS provides a normalised value of model accuracy so that it can be compared to accuracy by chance alone.

TSS values between 0.4 and 0.7 indicate a good model performance. Below this range, TSS indicates poor model performance, and above this range are models with excellent performance.  
  
```{r}
tss(best_mod_match_obs)
```
  
TSS suggests this model has excellent performance, TSS is about 2.5 times higher than the one obtained in MaxEnt, but once again, it is lower than the results for Random Forests (0.99).  
  
## Simplifying model
We will now check if we can remove the variables that contributed the least to the model (5% or less). The code below will remove one variable at a time, train the model and recalculate AUC. A variable will only be removed if it has no negative effect on predictive performance (using AUC).  
  
```{r}
reduced_model <- reduceVar(best_mod_match_obs, metric = "auc", test = model_data[[2]],
                          th = 5, permut = 10, use_jk = T)

reduced_model
```
  
No variables can be removed without compromising model performance. We will produce a final report for our model and finally predict crabeater distribution.  
  
## Model report
Before moving onto testing a new model, we will save a report with the information shown above.  
  
```{r, eval = F}
modelReport(best_mod_match_obs, folder = out_folder, test = model_data[[2]], 
            response_curves = T, only_presence = T, jk = T)
```
  
## Performance metrics
To be able to compare the performance of this model with the three other SDM algorithms to be used in the SDM ensemble, we will calculate three metrics: area under the receiver operating curve ($AUC_{ROC}$), area under the precisison-recall gain curve ($AUC_{PRG}$) and the Pearson correlation between the model predictions and the testing dataset.  
  
```{r}
#Predicting values using testing dataset
pred <- predict(best_mod_match_obs, model_data[[2]]@data, type = "response")

#AUC ROC
auc_roc <- auc(best_mod_match_obs, model_data[[2]])

#AUC PRG
auc_prg <- create_prg_curve(model_data[[2]]@pa, pred) %>% 
  calc_auprg()

#Pearson correlation
cor <- cor(pred, model_data[[2]]@pa)

print(c(paste0("AUC ROC: ", round(auc_roc, 3)),
        paste0("AUC PRG: ", round(auc_prg, 3)),
        paste0("Pearson correlation: ", round(cor, 3))))
```  
  
Saving model evaluation results.   

```{r, eval = F}
#Load model evaluation data frame and add results
model_eval_path <- "../../SDM_outputs/model_evaluation.csv"
read_csv(model_eval_path) %>% 
  bind_rows(data.frame(model = "BoostedRegressionTrees", env_trained = "mod_match_obs", auc_roc = auc_roc, 
                       auc_prg = auc_prg, pear_cor = cor)) %>% 
  write_csv(model_eval_path)
```

## Predictions
We will use the reduced model to predict crabeater seals distribution.  
  
```{r}
pred_mod_match_obs <- mean_model_baked %>% 
  drop_na() %>% 
  mutate(pred = as.vector(predict(reduced_model,
                                  data = drop_na(mean_model_baked),
                                  type = "response")))

pred_mod_match_obs_ras <- pred_mod_match_obs %>% 
  #Select relevant variables only
  select(xt_ocean, yt_ocean, pred, month) %>% 
  right_join(mean_model_baked %>%
  #Select relevant variables only
  select(xt_ocean, yt_ocean, month)) %>% 
  #Set dimensions
  st_as_stars(dims = c("xt_ocean", "yt_ocean", "month")) %>% 
  #Ensuring month dimension is shown correctly
  st_set_dimensions("month", values = c(11, 12)) %>%
  #Set CRS
  st_set_crs(4326) %>% 
  #Transform to South Pole stereographic
  st_transform(crs = st_crs(3976))

#Saving outputs
#Data frame
pred_mod_match_obs %>% 
  write_csv(file.path(out_folder, "mean_pred_match_obs.csv"))
#Saving as R dataset so it can be easily open with readRDS
saveRDS(pred_mod_match_obs_ras,
        file.path(out_folder, "mean_pred_match_obs_raster.rds"))
```
  
### Plotting predictions

```{r, eval = F}
#Plotting November distribution
#Prepping data
nov <- pred_mod_match_obs_ras %>% 
  slice(index = 1, along = "month") 

#Plotting
nov_plot <- ggplot()+
  geom_stars(data = nov)+
  geom_sf(data = antarctica)+
  lims(x = c(0, 4000000))+
  #Set colour palette
  scale_fill_cmocean(name = "haline", direction = -1, 
                     guide = guide_colorbar(barwidth = 1, barheight = 10, 
                                            ticks = FALSE, nbin = 1000, 
                                            frame.colour = "black"), 
                     limits = c(0, 1)) +
  theme_linedraw() +
  theme(panel.background = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        legend.position = "none",
        plot.title = element_text(hjust = 0.5)) +
  labs(title = "November",
       x = "Longitude",
       y = "Latitude")

dec <- pred_mod_match_obs_ras %>% 
  slice(index = 2, along = "month") 

dec_plot <- ggplot() +
  geom_stars(data = dec) +
  geom_sf(data = antarctica)+
  lims(x = c(0, 4000000))+
  scale_fill_cmocean(name = "haline", direction = -1, 
                     guide = guide_colorbar(barwidth = 1, barheight = 10, 
                                            ticks = FALSE, nbin = 1000, 
                                            frame.colour = "black"), 
                     limits = c(0, 1)) +
  theme_linedraw() +
  theme(panel.background = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        plot.title = element_text(hjust = 0.5)) +
  labs(title = "December",
       x = "Longitude",
       y = " ",
       fill = "Probability")

#Get legend
legend <- get_legend(dec_plot)

#Remove legend from December plot
dec_plot <- dec_plot + theme(legend.position = "none")

#Plotting together
plot_match_obs <- plot_grid(nov_plot, dec_plot, legend, ncol = 3, nrow = 1,
                            rel_widths = c(1, 1, 0.3))

#Add title
title <- ggdraw()+
  draw_label("Mean crabeater seal distribution\n(ACCESS-OM2-01 - simplified)",
             fontface = "bold", hjust = 0.5)+
  theme(plot.margin = ggplot2::margin(0, 0, 0, 0))

#Putting everything together
final <- plot_grid(title, plot_match_obs, ncol = 1, rel_heights = c(0.1, 1))

#Saving graph
ggsave(file.path(out_folder, "map_mean_pred_match_obs.png"), plot = final, 
       device = "png", bg = "white", width = 8.75, height = 7)
```
  
## ACCESS-OM2-01 - All variables
We will set up the output folder and load the ACCESS-OM2-01 data. Since BRTs are not affected by multicollinearity, we will use all variables available.  
  
```{r}
#Location of folder for outputs
out_folder <- "../../SDM_outputs/BoostedRegressionTrees/Mod_full"
#If folder does not exist, create one
if(!dir.exists(out_folder)){
  dir.create(out_folder, recursive = T)
}

#Loading data
mod_data <- read_csv(str_subset(file_list, "model")) %>% 
  select(!krill_growth_rate) %>% 
  #Setting month as factor and ordered factor
  mutate(month = as.factor(month))

# Splitting into testing and training
mod <- prep_data(mod_data, cat_vars, split = F)

#Applying SWD format to model data
model_data <- mod %>% 
  select(!year) %>% 
  drop_na(krill_ggp) %>% 
  sdm_format() %>% 
  trainValTest(test = 0.25, only_presence = T, seed = 42)
```
  
## Training BRT (full suite of ACCESS-OM2-01 variables)

```{r, eval = FALSE}
#Train model
default_model <- train(method = "BRT", data = model_data[[1]])

#Find number of predictors/features used
n_features <- ncol(model_data[[1]]@data)

# Define the hyperparameters to test
hyp_parm <- list(n.trees = seq(n_features*10, n_features*100, length.out = 10),
                 #maximum depth per tree
                 interaction.depth = 1:10,
                 #learning rate
                 shrinkage = c(1e-4, 1e-3, 1e-2, 0.1, 0.2, 0.3, 0.4, 0.5, 1),
                 bag.fraction = seq(0.25, 1, by = 0.25))

# Genetic algorithm that searches for best model parameters
optimised_model <- optimizeModel(default_model, hypers = hyp_parm, metric = "auc", 
                                 test = model_data[[2]], seed = 42)

#We will keep the best performing models based on AUC
best_mod <- optimised_model@models[[1]]

#Save model
best_mod %>% 
  saveRDS(file.path(out_folder, "best_BRT_ACCESS.rds"))

#Check model parameters
best_mod
```
  
```{r, echo = F}
#Loading best initial model to avoid performing grid search of parameters
best_mod <- readRDS(file.path(out_folder, "best_BRT_ACCESS.rds"))
best_mod
```
  
## Variable importance
  
```{r}
#Calculating importance
var_imp_mod <- varImp(best_mod)
#Plotting results
plotVarImp(var_imp_mod)
```
  
Krill habitat (`krill_ggp`) was the most influential variable for this model, `SIC` was among the top four, while `SST` had a relative low contribution ($< 5 \%$). Once again, `month` did not contribute at all to model performance. Next, we will check performance by plotting ROC curves and TSS value.    
  
## Jacknife test
We can check which environment variable contributes the most/least to the BRT results when testing against the training and testing datasets.  

```{r}
jk_mod <- doJk(best_mod, metric = "auc", test = model_data[[2]])
jk_mod
```
  
### Plotting Jacknife results
Results calculated from training dataset.  
  
```{r}
plotJk(jk_mod, type = "train", ref = auc(best_mod))
```
  
The `month`, long-term presence of pack ice (`lt_pack_ice`) and the slope of the seafloor (`bottom_slope_deg`) are once again the three variables that contribute the least when used on their own, which coincides with results from the model trained with the reduced environmental variables above. However, they contribute at least to half of the model performance, so we may not be able to exclude them to simplify this model. We can now check results from the testing dataset.  
  
```{r}
plotJk(jk_mod, type = "test", ref = auc(best_mod, test = model_data[[2]]))
```
  
Results from the testing dataset show that most variables contribute to model performance in a similar proportion. It appears that the exclusion of any of these variables will not significantly impact model performance, so we will test if it is possible to simplify the model. However, we may need to check the outputs of the simplify models as all variables contribute to at least half of the model performance when used on their own.    
  
## ROC curves
  
```{r}
plotROC(best_mod, test = model_data[[2]])
```
  
Based on AUC values, BRT is the best performing SDM algorithm.  
  
## TSS
This is a measure of predictive accuracy, which captures the proportion of correctly classified cells as true absences (specificity) or true presences (sensitivity). The TSS provides a normalised value of model accuracy so that it can be compared to accuracy by chance alone.

TSS values between 0.4 and 0.7 indicate a good model performance. Below this range, TSS indicates poor model performance, and above this range are models with excellent performance.  
  
```{r}
tss(best_mod)
```
  
TSS is more than double than the one obtained in MaxEnt (0.43), but slightly lower than Random Forest (0.982). This result suggests this model has an excellent performance.  
  
## Simplifying model
We will now check if we can remove the variables that contributed the least to the model (5% or less). The code below will remove one variable at a time, train the model and recalculate AUC. A variable will only be removed if it has no negative effect on predictive performance (using AUC).  
  
```{r}
reduced_model <- reduceVar(best_mod, metric = "auc", test = model_data[[2]],
                          th = 5, permut = 10, use_jk = T)

reduced_model
```
  
In total, eight variables were deemed to be redundant, including: bottom velocity of the water along longitudes (`vel_lon_bottom_msec`) and water velocity along latitudes (`vel_lat_bottom_msec` and `vel_lat_surf_msec`), the slope of the seafloor (`bottom_slope_deg`), `month`, long-term presence of sea ice (`lt_pack_ice`), water temperature near the seafloor (`bottom_temp_degC`), and depth (`depth_m`). We will check that the AUC and TSS are similar to the full model before moving onto creating predictions.  
  
```{r}
print(c(paste0("AUC: ", auc(reduced_model)),
        paste0("TSS: ", tss(reduced_model))))
```
  
Although the performance metrics are similar to those from the full model, we will use the full model instead because the predictions obtained with the simplified model were largely zero.  
  
## Model report
Before moving onto testing a new model, we will save a report with the information shown above.  
  
```{r, eval = F}
modelReport(best_mod, folder = out_folder, test = model_data[[2]], 
            response_curves = T, only_presence = T, jk = T)
```
  
## Performance metrics
To be able to compare the performance of this model with the three other SDM algorithms to be used in the SDM ensemble, we will calculate three metrics: area under the receiver operating curve ($AUC_{ROC}$), area under the precisison-recall gain curve ($AUC_{PRG}$) and the Pearson correlation between the model predictions and the testing dataset.  
  
```{r}
#Predicting values using testing dataset
pred <- predict(best_mod, model_data[[2]]@data, type = "response")

#AUC ROC
auc_roc <- auc(best_mod, model_data[[2]])

#AUC PRG
auc_prg <- create_prg_curve(model_data[[2]]@pa, pred) %>% 
  calc_auprg()

#Pearson correlation
cor <- cor(pred, model_data[[2]]@pa)

print(c(paste0("AUC ROC: ", round(auc_roc, 3)),
        paste0("AUC PRG: ", round(auc_prg, 3)),
        paste0("Pearson correlation: ", round(cor, 3))))
```  
  
Saving model evaluation results.   

```{r, eval = F}
#Load model evaluation data frame and add results
read_csv(model_eval_path) %>% 
  bind_rows(data.frame(model = "BoostedRegressionTrees", env_trained = "full_access", auc_roc = auc_roc, 
                       auc_prg = auc_prg, pear_cor = cor)) %>% 
  write_csv(model_eval_path)
```
  
## Predictions
We will use the reduced model to predict crabeater seals distribution.  
  
```{r}
pred_mod <- mean_model_baked %>% 
  drop_na() %>% 
  mutate(pred = as.vector(predict(best_mod,
                                  data = drop_na(mean_model_baked),
                                  type = "response")))

pred_mod_ras <- pred_mod %>% 
  #Select relevant variables only
  select(xt_ocean, yt_ocean, pred, month) %>% 
  right_join(mean_model_baked %>%
  #Select relevant variables only
  select(xt_ocean, yt_ocean, month)) %>% 
  #Set dimensions
  st_as_stars(dims = c("xt_ocean", "yt_ocean", "month")) %>% 
  #Ensuring month dimension is shown correctly
  st_set_dimensions("month", values = c(11, 12)) %>%
  #Set CRS
  st_set_crs(4326) %>% 
  #Transform to South Pole stereographic
  st_transform(crs = st_crs(3976))

#Saving outputs
#Data frame
pred_mod %>% 
  write_csv(file.path(out_folder, "mean_pred_ACCESS.csv"))
#Saving as R dataset so it can be easily open with readRDS
saveRDS(pred_mod_ras,
        file.path(out_folder, "mean_pred_ACCESS_raster.rds"))
```
  
### Plotting predictions

```{r, eval = F}
#Plotting November distribution
#Prepping data
nov <- pred_mod_ras %>% 
  slice(index = 1, along = "month") 

#Plotting
nov_plot <- ggplot()+
  geom_stars(data = nov)+
  geom_sf(data = antarctica)+
  lims(x = c(0, 4000000))+
  #Set colour palette
  scale_fill_cmocean(name = "haline", direction = -1, 
                     guide = guide_colorbar(barwidth = 1, barheight = 10, 
                                            ticks = FALSE, nbin = 1000, 
                                            frame.colour = "black"), 
                     limits = c(0, 1)) +
  theme_linedraw() +
  theme(panel.background = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        legend.position = "none",
        plot.title = element_text(hjust = 0.5)) +
  labs(title = "November",
       x = "Longitude",
       y = "Latitude")

dec <- pred_mod_ras %>% 
  slice(index = 2, along = "month") 

dec_plot <- ggplot() +
  geom_stars(data = dec) +
  geom_sf(data = antarctica)+
  lims(x = c(0, 4000000))+
  scale_fill_cmocean(name = "haline", direction = -1, 
                     guide = guide_colorbar(barwidth = 1, barheight = 10, 
                                            ticks = FALSE, nbin = 1000, 
                                            frame.colour = "black"), 
                     limits = c(0, 1)) +
  theme_linedraw() +
  theme(panel.background = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        plot.title = element_text(hjust = 0.5)) +
  labs(title = "December",
       x = "Longitude",
       y = " ",
       fill = "Probability")

#Get legend
legend <- get_legend(dec_plot)

#Remove legend from December plot
dec_plot <- dec_plot + theme(legend.position = "none")

#Plotting together
plot_match_obs <- plot_grid(nov_plot, dec_plot, legend, ncol = 3, nrow = 1,
                            rel_widths = c(1, 1, 0.3))

#Add title
title <- ggdraw()+
  draw_label("Mean crabeater seal distribution\n(ACCESS-OM2-01)",
             fontface = "bold", hjust = 0.5)+
  theme(plot.margin = ggplot2::margin(0, 0, 0, 0))

#Putting everything together
final <- plot_grid(title, plot_match_obs, ncol = 1, rel_heights = c(0.1, 1))

#Saving graph
ggsave(file.path(out_folder, "map_mean_pred_ACCESS.png"), plot = final, 
       device = "png", bg = "white", width = 8.75, height = 7)
```
    
## Observations (remotely sensed environmental variables)
We will set up a new output folder and load the remotely sensed environmental data. Since BRT are not affected by multicollinearity, we will use all variables available.  
  
```{r}
#Location of folder for outputs
out_folder <- "../../SDM_outputs/BoostedRegressionTrees/Obs"
#If folder does not exist, create one
if(!dir.exists(out_folder)){
  dir.create(out_folder, recursive = T)
}

#Loading mean environmental conditions from observations
mean_obs <- read_csv("../../Environmental_Data/Env_obs/All_values_month_Obs_env_vars.csv") %>% 
  mutate(month = as.factor(month))
#Preparing data to be used for prediction
mean_obs_baked <- prep_pred(mean_obs, "month")

#Loading data
obs_data <- read_csv(str_subset(file_list, "/obs")) %>% 
  #Setting month as factor and ordered factor
  mutate(month = as.factor(month))

# Splitting into testing and training
obs_data <- prep_data(obs_data, cat_vars, split = F)

#Applying SWD format to model data
model_data <- obs_data %>% 
  select(!year) %>% 
  sdm_format() %>% 
  trainValTest(test = 0.25, only_presence = T, seed = 42)
```
  
## Training BRT (observations)

```{r, eval = FALSE}
#Train model
default_model <- train(method = "BRT", data = model_data[[1]])

#Find number of predictors/features used
n_features <- ncol(model_data[[1]]@data)

# Define the hyperparameters to test
hyp_parm <- list(n.trees = seq(n_features*10, n_features*100, length.out = 10),
                 #maximum depth per tree
                 interaction.depth = 1:10,
                 #learning rate
                 shrinkage = c(1e-4, 1e-3, 1e-2, 0.1, 0.2, 0.3, 0.4, 0.5, 1),
                 bag.fraction = seq(0.25, 1, by = 0.25))

# Genetic algorithm that searches for best model parameters
optimised_model <- optimizeModel(default_model, hypers = hyp_parm, metric = "auc", 
                                 test = model_data[[2]], seed = 42)

#We will keep the best performing models based on AUC
best_obs <- optimised_model@models[[1]]

#Save model
best_obs %>% 
  saveRDS(file.path(out_folder, "best_BRT_obs.rds"))

#Check model parameters
best_obs
```
  
```{r, echo = F}
#Loading best initial model to avoid performing grid search of parameters
best_obs <- readRDS(file.path(out_folder, "best_BRT_obs.rds"))
best_obs
```
  
## Variable importance
  
```{r}
#Calculating importance
var_imp_obs <- varImp(best_obs)
#Plotting results
plotVarImp(var_imp_obs)
```
  
`SIC` was the most important variables identified in the model, which is similar to other models in the ensemble. `month` was the least importance, followed by slope of the seafloor (`bottom_slope_deg`) which contributed less than 5% each to model performance. Next, we will check performance by plotting ROC curves and TSS value.    
  
## Jacknife test
We can check which environment variable contributes the most/least to the BRT results when testing against the training and testing datasets.  

```{r}
jk_obs <- doJk(best_obs, metric = "auc", test = model_data[[2]])
jk_obs
```
  
### Plotting Jacknife results
Results calculated from training dataset.  
  
```{r}
plotJk(jk_obs, type = "train", ref = auc(best_obs))
```
  
The `month`, long-term presence of pack ice (`lt_pack_ice`) and the slope of the seafloor (`bottom_slope_deg`) are once again the three variables that contribute the least when used on their own, which coincides with results from the model trained above. We can now check results from the testing dataset.  
  
```{r}
plotJk(jk_obs, type = "test", ref = auc(best_obs, test = model_data[[2]]))
```
  
Results from the testing dataset show that most variables contribute to model performance in a similar proportion. The `month`, long-term presence of pack ice (`lt_pack_ice`) and the slope of the seafloor (`bottom_slope_deg`) continue to be the variables that contribute the least to the model. While the highest contributors are distance to the coastline (`dist_coast_km`), to the sea ice edge (`dist_ice_edge_km`) and to the continental shelf (`dist_shelf_km`). We will not check if it is possible to simplify the model as we show above that when variables contribute in similar manner, excluding them produces results that do not reflect what we know about crabeater seals.  
  
## ROC curves
  
```{r}
plotROC(best_obs, test = model_data[[2]])
```
  
Once again, BRT perfomance is slight lower than that of Random Forest (AUC test = 0.92), but about a third better than MaxEnt (AUC test = 0.67).
  
## TSS
This is a measure of predictive accuracy, which captures the proportion of correctly classified cells as true absences (specificity) or true presences (sensitivity). The TSS provides a normalised value of model accuracy so that it can be compared to accuracy by chance alone.

TSS values between 0.4 and 0.7 indicate a good model performance. Below this range, TSS indicates poor model performance, and above this range are models with excellent performance.  
  
```{r}
tss(best_obs)
```
  
This model has an excellent performance, although slightly lower than Random Forest (0.98), but more than double the score of MaxEnt (0.42).  
  
## Model report
Before moving onto testing a new model, we will save a report with the information shown above.  
  
```{r, eval = F}
modelReport(best_obs, folder = out_folder, test = model_data[[2]], 
            response_curves = T, only_presence = T, jk = T)
```
  
## Performance metrics
To be able to compare the performance of this model with the three other SDM algorithms to be used in the SDM ensemble, we will calculate three metrics: area under the receiver operating curve ($AUC_{ROC}$), area under the precisison-recall gain curve ($AUC_{PRG}$) and the Pearson correlation between the model predictions and the testing dataset.  
  
```{r}
#Predicting values using testing dataset
pred <- predict(best_obs, model_data[[2]]@data, type = "response")

#AUC ROC
auc_roc <- auc(best_obs, model_data[[2]])

#AUC PRG
auc_prg <- create_prg_curve(model_data[[2]]@pa, pred) %>% 
  calc_auprg()

#Pearson correlation
cor <- cor(pred, model_data[[2]]@pa)

print(c(paste0("AUC ROC: ", round(auc_roc, 3)),
        paste0("AUC PRG: ", round(auc_prg, 3)),
        paste0("Pearson correlation: ", round(cor, 3))))
```  
  
Saving model evaluation results.   

```{r, eval = F}
#Load model evaluation data frame and add results
read_csv(model_eval_path) %>% 
  bind_rows(data.frame(model = "BoostedRegressionTrees", env_trained = "observations", auc_roc = auc_roc, 
                       auc_prg = auc_prg, pear_cor = cor)) %>% 
  write_csv(model_eval_path)
```
  
## Predictions
We will use the reduced model to predict crabeater seals distribution.  
  
```{r}
pred_obs <- mean_obs_baked %>% 
  drop_na() %>% 
  mutate(pred = as.vector(predict(best_obs,
                                  data = drop_na(mean_obs_baked),
                                  type = "response")))

pred_obs_ras <- pred_obs %>% 
  #Select relevant variables only
  select(xt_ocean, yt_ocean, pred, month) %>% 
  right_join(mean_obs_baked %>%
  #Select relevant variables only
  select(xt_ocean, yt_ocean, month)) %>% 
  #Set dimensions
  st_as_stars(dims = c("xt_ocean", "yt_ocean", "month")) %>% 
  #Ensuring month dimension is shown correctly
  st_set_dimensions("month", values = c(11, 12)) %>%
  #Set CRS
  st_set_crs(4326) %>% 
  #Transform to South Pole stereographic
  st_transform(crs = st_crs(3976))

#Saving outputs
#Data frame
pred_obs %>% 
  write_csv(file.path(out_folder, "mean_pred_obs.csv"))
#Saving as R dataset so it can be easily open with readRDS
saveRDS(pred_obs_ras,
        file.path(out_folder, "mean_pred_obs_raster.rds"))
```
  
### Plotting predictions

```{r, eval = F}
#Plotting November distribution
#Prepping data
nov <- pred_obs_ras %>% 
  slice(index = 1, along = "month") 

#Plotting
nov_plot <- ggplot()+
  geom_stars(data = nov)+
  geom_sf(data = antarctica)+
  lims(x = c(0, 4000000))+
  #Set colour palette
  scale_fill_cmocean(name = "haline", direction = -1, 
                     guide = guide_colorbar(barwidth = 1, barheight = 10, 
                                            ticks = FALSE, nbin = 1000, 
                                            frame.colour = "black"), 
                     limits = c(0, 1)) +
  theme_linedraw() +
  theme(panel.background = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        legend.position = "none",
        plot.title = element_text(hjust = 0.5)) +
  labs(title = "November",
       x = "Longitude",
       y = "Latitude")

dec <- pred_obs_ras %>% 
  slice(index = 2, along = "month") 

dec_plot <- ggplot() +
  geom_stars(data = dec) +
  geom_sf(data = antarctica)+
  lims(x = c(0, 4000000))+
  scale_fill_cmocean(name = "haline", direction = -1, 
                     guide = guide_colorbar(barwidth = 1, barheight = 10, 
                                            ticks = FALSE, nbin = 1000, 
                                            frame.colour = "black"), 
                     limits = c(0, 1)) +
  theme_linedraw() +
  theme(panel.background = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(), 
        plot.title = element_text(hjust = 0.5)) +
  labs(title = "December",
       x = "Longitude",
       y = " ",
       fill = "Probability")

#Get legend
legend <- get_legend(dec_plot)

#Remove legend from December plot
dec_plot <- dec_plot + theme(legend.position = "none")

#Plotting together
plot_match_obs <- plot_grid(nov_plot, dec_plot, legend, ncol = 3, nrow = 1,
                            rel_widths = c(1, 1, 0.3))

#Add title
title <- ggdraw()+
  draw_label("Mean crabeater seal distribution\n(observations)",
             fontface = "bold", hjust = 0.5)+
  theme(plot.margin = ggplot2::margin(0, 0, 0, 0))

#Putting everything together
final <- plot_grid(title, plot_match_obs, ncol = 1, rel_heights = c(0.1, 1))

#Saving graph
ggsave(file.path(out_folder, "map_mean_pred_obs.png"), plot = final, 
       device = "png", bg = "white", width = 8.75, height = 7)
```